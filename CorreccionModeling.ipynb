{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: click in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: wrapt in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk gensim tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de utilidad para procesamiento de texto\n",
    "def analyze_text_metrics(text):\n",
    "    word_count = len(text.split(\" \"))\n",
    "    sentence_count = len([sent for sent in re.split(r'[.]', text) if sent.strip()])\n",
    "    return word_count, sentence_count\n",
    "\n",
    "# Cargar datos\n",
    "podcast_data = pd.read_csv('data/podcastdata_dataset.csv')\n",
    "\n",
    "# Calcular métricas básicas\n",
    "podcast_data['word_count'], podcast_data['sentence_count'] = zip(*podcast_data['text'].apply(analyze_text_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET ORIGINAL ===\n",
      "      id             guest                                              title  \\\n",
      "0      1       Max Tegmark                                           Life 3.0   \n",
      "1      2     Christof Koch                                      Consciousness   \n",
      "2      3     Steven Pinker                            AI in the Age of Reason   \n",
      "3      4     Yoshua Bengio                                      Deep Learning   \n",
      "4      5   Vladimir Vapnik                               Statistical Learning   \n",
      "..   ...               ...                                                ...   \n",
      "314  321      Ray Kurzweil    Singularity, Superintelligence, and Immortality   \n",
      "315  322  Rana el Kaliouby   Emotion AI, Social Robots, and Self-Driving Cars   \n",
      "316  323        Will Sasso  Comedy, MADtv, AI, Friendship, Madness, and Pr...   \n",
      "317  324   Daniel Negreanu                                              Poker   \n",
      "318  325     Michael Levin  Biology, Life, Aliens, Evolution, Embryogenesi...   \n",
      "\n",
      "                                                  text  word_count  \\\n",
      "0    As part of MIT course 6S099, Artificial Genera...       13424   \n",
      "1    As part of MIT course 6S099 on artificial gene...       10217   \n",
      "2    You've studied the human mind, cognition, lang...        5989   \n",
      "3    What difference between biological neural netw...        5993   \n",
      "4    The following is a conversation with Vladimir ...        6374   \n",
      "..                                                 ...         ...   \n",
      "314  By the time he gets to 2045, we'll be able to ...       12807   \n",
      "315  there's a broader question here, right? As we ...       26034   \n",
      "316  Once this whole thing falls apart and we are c...       25255   \n",
      "317  you could be the seventh best player in the wh...       29911   \n",
      "318  turns out that if you train a planarian and th...       33714   \n",
      "\n",
      "     sentence_count  \n",
      "0               610  \n",
      "1               496  \n",
      "2               289  \n",
      "3               308  \n",
      "4               483  \n",
      "..              ...  \n",
      "314             851  \n",
      "315            1676  \n",
      "316            1778  \n",
      "317            1649  \n",
      "318            1707  \n",
      "\n",
      "[319 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Visualizar el DataFrame inicial\n",
    "print(\"=== DATASET ORIGINAL ===\")\n",
    "print(podcast_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(text):\n",
    "    \"\"\"Extrae oraciones limpias del texto.\"\"\"\n",
    "    return [sentence.strip() for sentence in re.split(r'[.]', text) if sentence.strip()]\n",
    "\n",
    "# Crear dataset de oraciones\n",
    "sentence_records = []\n",
    "for index, row in podcast_data.iterrows():\n",
    "    podcast_id = row['id']\n",
    "    episode_sentences = extract_sentences(row['text'])\n",
    "    \n",
    "    for sent_num, sentence in enumerate(episode_sentences, 1):\n",
    "        sentence_records.append({\n",
    "            'podcast_id': podcast_id, \n",
    "            'sentence_number': sent_num, \n",
    "            'content': sentence\n",
    "        })\n",
    "\n",
    "sentence_dataset = pd.DataFrame(sentence_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET DE ORACIONES ===\n",
      "        podcast_id  sentence_number  \\\n",
      "0                1                1   \n",
      "1                1                2   \n",
      "2                1                3   \n",
      "3                1                4   \n",
      "4                1                5   \n",
      "...            ...              ...   \n",
      "385890         325             1703   \n",
      "385891         325             1704   \n",
      "385892         325             1705   \n",
      "385893         325             1706   \n",
      "385894         325             1707   \n",
      "\n",
      "                                                  content  \\\n",
      "0       As part of MIT course 6S099, Artificial Genera...   \n",
      "1                           He is a professor here at MIT   \n",
      "2       He's a physicist, spent a large part of his ca...   \n",
      "3       But he's also studied and delved into the bene...   \n",
      "4       Amongst many other things, he is the cofounder...   \n",
      "...                                                   ...   \n",
      "385890                                 It's the beginning   \n",
      "385891  It's not the whole story by any means, but it'...   \n",
      "385892  Where's state stored of the system? Is it in p...   \n",
      "385893                     So there are chemical networks   \n",
      "385894  So for example, gene regulatory networks, righ...   \n",
      "\n",
      "                                    vector_representation  relevance_score  \\\n",
      "0       [-0.24118938, -0.80681926, 0.26220858, -0.0111...         0.311911   \n",
      "1       [0.32014036, 0.13129307, -0.6110095, -0.331558...        -0.081108   \n",
      "2       [0.09415934, -0.023158098, -0.013525311, -0.15...         0.129232   \n",
      "3       [0.07151673, -0.45670554, 0.39938843, -0.48070...         0.297525   \n",
      "4       [-0.28297734, -0.23709355, -0.26427576, -0.098...         0.210976   \n",
      "...                                                   ...              ...   \n",
      "385890  [-0.48295125, -0.09778154, 0.88929456, -1.0210...         0.095970   \n",
      "385891  [0.12049872, -0.11247781, 1.2341539, -0.415682...         0.180469   \n",
      "385892  [0.18327036, -0.12184353, 0.72429204, -0.39837...         0.355526   \n",
      "385893  [-0.6361799, -0.0375051, 1.2611163, -1.4623897...         0.700362   \n",
      "385894  [-0.10411927, 0.19346975, 0.57143694, -0.14876...         0.567625   \n",
      "\n",
      "        topic_id  \n",
      "0              3  \n",
      "1              1  \n",
      "2              9  \n",
      "3              9  \n",
      "4              7  \n",
      "...          ...  \n",
      "385890         9  \n",
      "385891         9  \n",
      "385892         9  \n",
      "385893         7  \n",
      "385894         9  \n",
      "\n",
      "[385895 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Visualizar el DataFrame de oraciones\n",
    "print(\"=== DATASET DE ORACIONES ===\")\n",
    "print(sentence_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar oraciones para Word2Vec\n",
    "processed_sentences = [simple_preprocess(sent) for sent in sentence_dataset['content']]\n",
    "\n",
    "# Entrenar modelo de embeddings\n",
    "embedding_model = Word2Vec(\n",
    "    sentences=processed_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# Generar vectores de oraciones\n",
    "def generate_sentence_vector(sentence):\n",
    "    tokens = simple_preprocess(sentence)\n",
    "    if not tokens:\n",
    "        return np.zeros(100)\n",
    "    return embedding_model.wv[tokens].mean(axis=0)\n",
    "\n",
    "sentence_dataset['vector_representation'] = sentence_dataset['content'].apply(generate_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query_text, model, data_frame):\n",
    "    \"\"\"Realiza búsqueda semántica en el conjunto de oraciones.\"\"\"\n",
    "    # Generar embedding de la consulta\n",
    "    query_tokens = simple_preprocess(query_text)\n",
    "    try:\n",
    "        query_vector = np.mean([model.wv[token] for token in query_tokens if token in model.wv], axis=0)\n",
    "    except:\n",
    "        data_frame['relevance_score'] = 0.0\n",
    "        return data_frame\n",
    "    \n",
    "    # Calcular similitudes\n",
    "    similarity_scores = []\n",
    "    for vec in tqdm(data_frame['vector_representation'], desc=\"Evaluando similitud\"):\n",
    "        score = cosine_similarity([query_vector], [vec])[0][0] if not np.all(vec == 0) else 0.0\n",
    "        similarity_scores.append(score)\n",
    "    \n",
    "    data_frame['relevance_score'] = similarity_scores\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando similitud: 100%|██████████| 385895/385895 [03:00<00:00, 2143.54it/s]\n"
     ]
    }
   ],
   "source": [
    "search_query = \"biological neural networks\"\n",
    "sentence_dataset = semantic_search(search_query, embedding_model, sentence_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar vectores para clustering\n",
    "sentence_vectors = np.array(sentence_dataset['vector_representation'].tolist())\n",
    "\n",
    "# Configurar y ejecutar clustering\n",
    "topic_count = 10\n",
    "topic_model = KMeans(n_clusters=topic_count, random_state=42, n_init=10, max_iter=300)\n",
    "sentence_dataset['topic_id'] = topic_model.fit_predict(sentence_vectors)\n",
    "\n",
    "# Generar representaciones de tópicos\n",
    "topic_representations = []\n",
    "for topic_idx in range(topic_count):\n",
    "    topic_vectors = sentence_vectors[sentence_dataset['topic_id'] == topic_idx]\n",
    "    topic_centroid = topic_vectors.mean(axis=0)\n",
    "    topic_representations.append({\n",
    "        'topic_number': topic_idx + 1,\n",
    "        'topic_vector': topic_centroid\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de tópicos\n",
    "topics_summary = pd.DataFrame(topic_representations)\n",
    "\n",
    "# DataFrame de episodios y tópicos\n",
    "episode_topics = sentence_dataset.groupby(['podcast_id', 'topic_id']).first().reset_index()\n",
    "episode_topics = episode_topics[['podcast_id', 'topic_id', 'vector_representation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESUMEN DEL ANÁLISIS DE PODCAST ===\n",
      "\n",
      "Total de episodios analizados: 319\n",
      "Total de oraciones procesadas: 385895\n",
      "\n",
      "Estadísticas por episodio:\n",
      "         word_count  sentence_count\n",
      "count    319.000000      319.000000\n",
      "mean   21672.557994     1209.702194\n",
      "std     9961.707400      637.632089\n",
      "min     4928.000000      118.000000\n",
      "25%    14079.500000      719.000000\n",
      "50%    20334.000000     1128.000000\n",
      "75%    28266.500000     1545.000000\n",
      "max    59475.000000     3748.000000\n",
      "\n",
      "=== RESULTADOS DE BÚSQUEDA ===\n",
      "\n",
      "Consulta: 'biological neural networks'\n",
      "\n",
      "Top 5 oraciones más relevantes:\n",
      "\n",
      "Podcast ID: 103\n",
      "Relevancia: 0.9731\n",
      "Contenido: AKA neural networks\n",
      "\n",
      "Podcast ID: 186\n",
      "Relevancia: 0.9179\n",
      "Contenido: A neural surgeon\n",
      "\n",
      "Podcast ID: 20\n",
      "Relevancia: 0.8905\n",
      "Contenido: Well, neural networks and graphs\n",
      "\n",
      "Podcast ID: 49\n",
      "Relevancia: 0.8653\n",
      "Contenido: So it's like digital neural net will interface with biological neural net\n",
      "\n",
      "Podcast ID: 190\n",
      "Relevancia: 0.8488\n",
      "Contenido: I mean, neural networks themselves are fundamentally visual\n",
      "\n",
      "=== DISTRIBUCIÓN DE TÓPICOS ===\n",
      "\n",
      "Tópico 1: 85555 oraciones\n",
      "\n",
      "Tópico 2: 27013 oraciones\n",
      "\n",
      "Tópico 3: 16550 oraciones\n",
      "\n",
      "Tópico 4: 61500 oraciones\n",
      "\n",
      "Tópico 5: 3335 oraciones\n",
      "\n",
      "Tópico 6: 44873 oraciones\n",
      "\n",
      "Tópico 7: 36453 oraciones\n",
      "\n",
      "Tópico 8: 34765 oraciones\n",
      "\n",
      "Tópico 9: 9172 oraciones\n",
      "\n",
      "Tópico 10: 66679 oraciones\n",
      "\n",
      "=== RESUMEN DE TÓPICOS ===\n",
      "   topic_number                                       topic_vector\n",
      "0             1  [0.2716627314602237, -0.39068538414334186, 0.5...\n",
      "1             2  [0.1633726073762581, -0.23962342502059666, -0....\n",
      "2             3  [0.2569294721456778, -0.3469590314411693, 0.60...\n",
      "3             4  [0.04512638224892666, -0.3828645478438623, 0.1...\n",
      "4             5  [1.0328790128990628, 0.018046600371878455, -0....\n",
      "5             6  [0.34781263153890624, -0.33430286325759373, 0....\n",
      "6             7  [0.4514106029792217, -0.6600384762850724, 0.30...\n",
      "7             8  [0.13151785736256513, -0.12746989384760463, 0....\n",
      "8             9  [0.473225231372428, -0.5206972416535057, 0.604...\n",
      "9            10  [-0.06521919424666849, -0.24519458091385982, 0...\n",
      "\n",
      "=== ASIGNACIÓN DE TÓPICOS POR EPISODIO ===\n",
      "      podcast_id  topic_id                              vector_representation\n",
      "0              1         0  [0.42092088, 0.07256121, 0.02922535, -0.261475...\n",
      "1              1         1  [0.32014036, 0.13129307, -0.6110095, -0.331558...\n",
      "2              1         2  [0.28081134, 0.1528258, 0.6193211, -0.68868893...\n",
      "3              1         3  [-0.24118938, -0.80681926, 0.26220858, -0.0111...\n",
      "4              1         4  [1.066521, 0.2521375, 0.57191277, -1.0770845, ...\n",
      "...          ...       ...                                                ...\n",
      "3139         325         5  [0.80505145, -1.0130284, 0.3723187, 0.15774114...\n",
      "3140         325         6  [0.5971986, -0.2861386, 0.7278203, -0.3757989,...\n",
      "3141         325         7  [0.10436215, 0.01787713, 0.51007867, -1.027893...\n",
      "3142         325         8  [0.4687005, -0.955912, 0.6554848, -0.21946487,...\n",
      "3143         325         9  [-0.2704674, -0.5213708, 0.7925515, -0.4515211...\n",
      "\n",
      "[3144 rows x 3 columns]\n",
      "=== DATAFRAME DE TÓPICOS ===\n",
      "   topic_number                                       topic_vector\n",
      "0             1  [0.2716627314602237, -0.39068538414334186, 0.5...\n",
      "1             2  [0.1633726073762581, -0.23962342502059666, -0....\n",
      "2             3  [0.2569294721456778, -0.3469590314411693, 0.60...\n",
      "3             4  [0.04512638224892666, -0.3828645478438623, 0.1...\n",
      "4             5  [1.0328790128990628, 0.018046600371878455, -0....\n",
      "5             6  [0.34781263153890624, -0.33430286325759373, 0....\n",
      "6             7  [0.4514106029792217, -0.6600384762850724, 0.30...\n",
      "7             8  [0.13151785736256513, -0.12746989384760463, 0....\n",
      "8             9  [0.473225231372428, -0.5206972416535057, 0.604...\n",
      "9            10  [-0.06521919424666849, -0.24519458091385982, 0...\n",
      "=== DATAFRAME FINAL ===\n",
      "      podcast_id  topic_id                              vector_representation\n",
      "0              1         0  [0.42092088, 0.07256121, 0.02922535, -0.261475...\n",
      "1              1         1  [0.32014036, 0.13129307, -0.6110095, -0.331558...\n",
      "2              1         2  [0.28081134, 0.1528258, 0.6193211, -0.68868893...\n",
      "3              1         3  [-0.24118938, -0.80681926, 0.26220858, -0.0111...\n",
      "4              1         4  [1.066521, 0.2521375, 0.57191277, -1.0770845, ...\n",
      "...          ...       ...                                                ...\n",
      "3139         325         5  [0.80505145, -1.0130284, 0.3723187, 0.15774114...\n",
      "3140         325         6  [0.5971986, -0.2861386, 0.7278203, -0.3757989,...\n",
      "3141         325         7  [0.10436215, 0.01787713, 0.51007867, -1.027893...\n",
      "3142         325         8  [0.4687005, -0.955912, 0.6554848, -0.21946487,...\n",
      "3143         325         9  [-0.2704674, -0.5213708, 0.7925515, -0.4515211...\n",
      "\n",
      "[3144 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar resultados del análisis\n",
    "print(\"=== RESUMEN DEL ANÁLISIS DE PODCAST ===\")\n",
    "print(f\"\\nTotal de episodios analizados: {len(podcast_data)}\")\n",
    "print(f\"Total de oraciones procesadas: {len(sentence_dataset)}\")\n",
    "print(f\"\\nEstadísticas por episodio:\")\n",
    "print(podcast_data[['word_count', 'sentence_count']].describe())\n",
    "\n",
    "print(\"\\n=== RESULTADOS DE BÚSQUEDA ===\")\n",
    "print(f\"\\nConsulta: '{search_query}'\")\n",
    "print(\"\\nTop 5 oraciones más relevantes:\")\n",
    "top_results = sentence_dataset.nlargest(5, 'relevance_score')\n",
    "for _, row in top_results.iterrows():\n",
    "    print(f\"\\nPodcast ID: {row['podcast_id']}\")\n",
    "    print(f\"Relevancia: {row['relevance_score']:.4f}\")\n",
    "    print(f\"Contenido: {row['content']}\")\n",
    "\n",
    "print(\"\\n=== DISTRIBUCIÓN DE TÓPICOS ===\")\n",
    "topic_distribution = sentence_dataset['topic_id'].value_counts().sort_index()\n",
    "for topic_id, count in topic_distribution.items():\n",
    "    print(f\"\\nTópico {topic_id + 1}: {count} oraciones\")\n",
    "    \n",
    "# Mostrar los DataFrames finales\n",
    "print(\"\\n=== RESUMEN DE TÓPICOS ===\")\n",
    "print(topics_summary)\n",
    "\n",
    "print(\"\\n=== ASIGNACIÓN DE TÓPICOS POR EPISODIO ===\")\n",
    "print(episode_topics)\n",
    "\n",
    "# Visualizar DataFrame de tópicos (equivalente a topics_df original)\n",
    "print(\"=== DATAFRAME DE TÓPICOS ===\")\n",
    "print(topics_summary)\n",
    "\n",
    "# Visualizar DataFrame final (equivalente a final_df original)\n",
    "print(\"=== DATAFRAME FINAL ===\")\n",
    "print(episode_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
