{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\galot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del dataset:\n",
      "   id            guest                    title  \\\n",
      "0   1      Max Tegmark                 Life 3.0   \n",
      "1   2    Christof Koch            Consciousness   \n",
      "2   3    Steven Pinker  AI in the Age of Reason   \n",
      "3   4    Yoshua Bengio            Deep Learning   \n",
      "4   5  Vladimir Vapnik     Statistical Learning   \n",
      "\n",
      "                                                text  \n",
      "0  As part of MIT course 6S099, Artificial Genera...  \n",
      "1  As part of MIT course 6S099 on artificial gene...  \n",
      "2  You've studied the human mind, cognition, lang...  \n",
      "3  What difference between biological neural netw...  \n",
      "4  The following is a conversation with Vladimir ...  \n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Descargar recursos necesarios de nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Ruta del dataset\n",
    "ruta_dataset = r'podcastdata_dataset.csv'\n",
    "\n",
    "# Cargar el dataset\n",
    "datos = pd.read_csv(ruta_dataset)\n",
    "\n",
    "# Mostrar las primeras filas del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(datos.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de títulos únicos después de la limpieza: 317\n"
     ]
    }
   ],
   "source": [
    "# Limpiar espacios iniciales y finales en los títulos de los episodios\n",
    "datos['title'] = datos['title'].str.strip()\n",
    "\n",
    "# Contar títulos únicos\n",
    "titulos_unicos = datos['title'].nunique()\n",
    "print(f\"Número de títulos únicos después de la limpieza: {titulos_unicos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de palabras por episodio:\n",
      "   id                    title  conteo_palabras\n",
      "0   1                 Life 3.0            13424\n",
      "1   2            Consciousness            10217\n",
      "2   3  AI in the Age of Reason             5989\n",
      "3   4            Deep Learning             5993\n",
      "4   5     Statistical Learning             6374\n"
     ]
    }
   ],
   "source": [
    "# Agregar una columna con el conteo de palabras en la columna 'text'\n",
    "datos['conteo_palabras'] = datos['text'].apply(lambda texto: len(str(texto).split()))\n",
    "\n",
    "# Mostrar el conteo de palabras para los primeros episodios\n",
    "print(\"Conteo de palabras por episodio:\")\n",
    "print(datos[['id', 'title', 'conteo_palabras']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de oraciones por episodio:\n",
      "   id                    title  conteo_oraciones\n",
      "0   1                 Life 3.0               737\n",
      "1   2            Consciousness               603\n",
      "2   3  AI in the Age of Reason               325\n",
      "3   4            Deep Learning               359\n",
      "4   5     Statistical Learning               551\n"
     ]
    }
   ],
   "source": [
    "# Contar oraciones usando el tokenizador de oraciones de nltk\n",
    "datos['conteo_oraciones'] = datos['text'].apply(lambda texto: len(sent_tokenize(str(texto))))\n",
    "\n",
    "# Mostrar el conteo de oraciones para los primeros episodios\n",
    "print(\"Conteo de oraciones por episodio:\")\n",
    "print(datos[['id', 'title', 'conteo_oraciones']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detalles del episodio 1:\n",
      "Título: Life 3.0\n",
      "Palabras: 13424\n",
      "Oraciones: 737\n"
     ]
    }
   ],
   "source": [
    "# Solicitar al usuario el ID del episodio que desea analizar\n",
    "episodio_id = 1  # Cambiar por el ID deseado\n",
    "\n",
    "# Filtrar el episodio seleccionado\n",
    "episodio = datos[datos['id'] == episodio_id]\n",
    "\n",
    "if episodio.empty:\n",
    "    print(f\"No se encontró el episodio con ID {episodio_id}.\")\n",
    "else:\n",
    "    # Obtener detalles del episodio\n",
    "    titulo = episodio['title'].iloc[0]\n",
    "    conteo_palabras = episodio['conteo_palabras'].iloc[0]\n",
    "    conteo_oraciones = episodio['conteo_oraciones'].iloc[0]\n",
    "    \n",
    "    # Mostrar los detalles del episodio\n",
    "    print(f\"Detalles del episodio {episodio_id}:\")\n",
    "    print(f\"Título: {titulo}\")\n",
    "    print(f\"Palabras: {conteo_palabras}\")\n",
    "    print(f\"Oraciones: {conteo_oraciones}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del DataFrame de oraciones:\n",
      "   ep_id  st_id                                            oracion  \\\n",
      "0      1      1  As part of MIT course 6S099, Artificial Genera...   \n",
      "1      1      2                     He is a professor here at MIT.   \n",
      "2      1      3  He's a physicist, spent a large part of his ca...   \n",
      "3      1      4  But he's also studied and delved into the bene...   \n",
      "4      1      5  Amongst many other things, he is the cofounder...   \n",
      "\n",
      "   conteo_palabras  \n",
      "0               19  \n",
      "1                7  \n",
      "2               17  \n",
      "3               17  \n",
      "4               24  \n"
     ]
    }
   ],
   "source": [
    "# Procesar episodios y dividir cada texto en oraciones\n",
    "datos_procesados = []\n",
    "\n",
    "for _, fila in datos.iterrows():\n",
    "    ep_id = fila['id']\n",
    "    texto = str(fila['text'])\n",
    "    oraciones = sent_tokenize(texto)  # Dividir texto en oraciones\n",
    "    \n",
    "    for st_id, oracion in enumerate(oraciones, start=1):\n",
    "        datos_procesados.append({\n",
    "            'ep_id': ep_id,\n",
    "            'st_id': st_id,\n",
    "            'oracion': oracion,\n",
    "            'conteo_palabras': len(oracion.split())\n",
    "        })\n",
    "\n",
    "# Crear un nuevo DataFrame con las oraciones procesadas\n",
    "df_oraciones = pd.DataFrame(datos_procesados)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame procesado\n",
    "print(\"Primeras filas del DataFrame de oraciones:\")\n",
    "print(df_oraciones.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.48.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.27.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\galot\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\galot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar embeddings para todas las oraciones en bloques (batches)\n",
    "print(\"Generando embeddings en bloques. Esto será más rápido...\")\n",
    "\n",
    "# Convertir las oraciones a una lista\n",
    "oraciones = df_oraciones['oracion'].tolist()\n",
    "\n",
    "# Generar embeddings en lotes\n",
    "embeddings = modelo_embeddings.encode(oraciones, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Asignar los embeddings al DataFrame\n",
    "df_oraciones['embedding'] = embeddings.tolist()\n",
    "\n",
    "print(\"Embeddings generados exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\galot\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\galot\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\galot\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\galot\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando embeddings para las oraciones...\n"
     ]
    }
   ],
   "source": [
    "# Importar la librería de SentenceTransformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cargar el modelo preentrenado\n",
    "modelo_embeddings = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generar embeddings para cada oración (puede tardar un poco)\n",
    "print(\"Generando embeddings para las oraciones...\")\n",
    "df_oraciones['embedding'] = df_oraciones['oracion'].apply(lambda texto: modelo_embeddings.encode(texto).tolist())\n",
    "\n",
    "# Mostrar una muestra del DataFrame con embeddings generados\n",
    "print(\"Primeras filas con embeddings generados:\")\n",
    "print(df_oraciones.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame procesado con embeddings en un archivo CSV\n",
    "ruta_salida = r'oraciones_procesadas.csv'\n",
    "df_oraciones.to_csv(ruta_salida, index=False)\n",
    "\n",
    "print(f\"Los resultados se han guardado en el archivo: {ruta_salida}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
