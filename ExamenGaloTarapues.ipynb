{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Descargar recursos necesarios de NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class PreprocesamientoDatos:\n",
        "    def __init__(self):\n",
        "        # Configurar stopwords\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Inicializar lematizador\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def limpiar_texto(self, texto):\n",
        "        \"\"\"\n",
        "        Proceso completo de preprocesamiento de texto\n",
        "        \"\"\"\n",
        "        if not isinstance(texto, str):\n",
        "            return \"\"\n",
        "\n",
        "        # 1. Convertir a minúsculas\n",
        "        texto = texto.lower()\n",
        "\n",
        "        # 2. Eliminación de caracteres especiales\n",
        "        texto = re.sub(r'[^a-zA-Z\\s]', '', texto)\n",
        "\n",
        "        # 3. Tokenización\n",
        "        tokens = word_tokenize(texto)\n",
        "\n",
        "        # 4. Eliminación de stopwords\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "\n",
        "        # 5. Lematización\n",
        "        tokens_lematizados = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        return ' '.join(tokens_lematizados)\n",
        "\n",
        "class SistemaRecuperacion:\n",
        "    def __init__(self, archivo_csv_criticas, archivo_csv_peliculas):\n",
        "        self.archivo_csv_criticas = archivo_csv_criticas\n",
        "        self.archivo_csv_peliculas = archivo_csv_peliculas\n",
        "        self.reviews = self.cargar_datos(archivo_csv_criticas)\n",
        "        self.movies = self.cargar_datos(archivo_csv_peliculas)\n",
        "        self.preprocesador = PreprocesamientoDatos()\n",
        "        self.vectorizador = TfidfVectorizer()\n",
        "\n",
        "    def cargar_datos(self, archivo_csv):\n",
        "        # Cargar datos desde el archivo CSV\n",
        "        return pd.read_csv(archivo_csv)\n",
        "\n",
        "    def preprocesar_criticas(self):\n",
        "        # Preprocesar las críticas de las películas\n",
        "        self.reviews['review_preprocessed'] = self.reviews['review_content'].apply(\n",
        "            lambda texto: self.preprocesador.limpiar_texto(str(texto))\n",
        "        )\n",
        "\n",
        "    def indexar_criticas(self):\n",
        "        \"\"\"\n",
        "        Indexar las críticas mediante un vector de características (TF-IDF)\n",
        "        \"\"\"\n",
        "        # Usamos TF-IDF para convertir las críticas en vectores numéricos\n",
        "        self.tfidf_matrix = self.vectorizador.fit_transform(self.reviews['review_preprocessed'])\n",
        "\n",
        "    def procesar_consulta(self, consulta):\n",
        "        \"\"\"\n",
        "        Procesar la consulta predefinida\n",
        "        \"\"\"\n",
        "        # Preprocesar la consulta\n",
        "        consulta_preprocesada = self.preprocesador.limpiar_texto(consulta)\n",
        "\n",
        "        # Convertir la consulta a un vector usando el mismo vectorizador\n",
        "        return self.vectorizador.transform([consulta_preprocesada])\n",
        "\n",
        "    def recuperar_peliculas_relevantes(self, consulta_preprocesada):\n",
        "        \"\"\"\n",
        "        Recuperar las películas más relevantes basadas en la consulta predefinida\n",
        "        \"\"\"\n",
        "        # Calcular la similitud entre la consulta y las críticas\n",
        "        similitudes = cosine_similarity(consulta_preprocesada, self.tfidf_matrix).flatten()\n",
        "\n",
        "        # Ordenar las críticas por similitud (de mayor a menor)\n",
        "        indices_similares = similitudes.argsort()[::-1]\n",
        "\n",
        "        # Obtener las películas más relevantes\n",
        "        peliculas_relevantes = self.reviews.iloc[indices_similares]\n",
        "\n",
        "        # Mapear las películas a las críticas relevantes\n",
        "        peliculas_relevantes = peliculas_relevantes.merge(self.movies,\n",
        "                                                          left_on=\"rotten_tomatoes_link\",\n",
        "                                                          right_on=\"rotten_tomatoes_link\")\n",
        "\n",
        "        # Seleccionar solo las columnas relevantes (título y link)\n",
        "        peliculas_relevantes = peliculas_relevantes[['movie_title', 'rotten_tomatoes_link', 'review_content']]\n",
        "\n",
        "        return peliculas_relevantes\n",
        "\n",
        "# Ejemplo de uso\n",
        "def main():\n",
        "    # Definir las rutas de los archivos CSV\n",
        "    archivo_criticas = 'rotten_tomatoes_critic_reviews.csv'\n",
        "    archivo_peliculas = 'rotten_tomatoes_movies.csv'\n",
        "\n",
        "    # Iniciar el sistema de recuperación\n",
        "    sistema = SistemaRecuperacion(archivo_criticas, archivo_peliculas)\n",
        "\n",
        "    # Preprocesar las críticas\n",
        "    sistema.preprocesar_criticas()\n",
        "\n",
        "    # Indexar las críticas\n",
        "    sistema.indexar_criticas()\n",
        "\n",
        "    # Definir la consulta predefinida (por ejemplo, una consulta de fantasía y aventura)\n",
        "    consulta = \"A fantasy adventure movie\"\n",
        "\n",
        "    # Procesar la consulta\n",
        "    consulta_preprocesada = sistema.procesar_consulta(consulta)\n",
        "\n",
        "    # Recuperar las películas más relevantes\n",
        "    peliculas_relevantes = sistema.recuperar_peliculas_relevantes(consulta_preprocesada)\n",
        "\n",
        "    # Mostrar las películas relevantes\n",
        "    print(\"\\nPelículas más relevantes:\")\n",
        "    print(peliculas_relevantes.head(10))  # Mostrar las primeras 10 películas\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AeJIsM2wAQg",
        "outputId": "59f04d2a-eb92-4175-ad57-e41b4712ca5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}